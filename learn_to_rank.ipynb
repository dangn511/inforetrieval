{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f3c8400",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Lab:10\n",
    "\n",
    "Learning to Rank\n",
    "\n",
    "You are given a dataset (https://guzpenha.github.io/MANtIS/) with a set of queries and one relevant document per query.\n",
    "The code flow:\n",
    "    1)Read the .tsv file and store them in a list.\n",
    "    2)Create train-test-validation splits.\n",
    "    3)Tokenize the text.\n",
    "    4)Define the vocab\n",
    "    5)Dataloader:\n",
    "        5.1) define text processing pipelines, i.e., tokenize text, convert tokens to ids, padding.\n",
    "        5.2) define negative sampling, i.e., sample irrelevant documents.\n",
    "        5.3) define collate function for dataloader. It processes the text in batches and puts samples to GPU.\n",
    "    6)Define Ranking_model class. \n",
    "        ---> Complete Exercise 1,2,3, and 4 inside it.\n",
    "    7)Initialize model embedding with glove.\n",
    "    8)Define rank_docs function that takes in a query, a set of documents, and returns document scores.\n",
    "    9)Begin model training and evaluation.\n",
    "        ---> Complete Exercise 5.1 and Exercise 5.2 that defines the loss function.\n",
    "'''\n",
    "\n",
    "import pandas as pd \n",
    "import torch\n",
    "import random\n",
    "\n",
    "#find gpu otherwise use cpu\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "\n",
    "'''\n",
    "load and split the data\n",
    "'''\n",
    "data = pd.read_csv(\"data.tsv\", sep=\"\\t\")\n",
    "data.head()\n",
    "\n",
    "queries = data['context'].values.tolist()\n",
    "rel_docs = data['response'].values.tolist()\n",
    "\n",
    "data = [(queries[i], rel_docs[i]) for i in range(len(queries))]\n",
    "\n",
    "\n",
    "#Train-Test split\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_dataset, test_dataset = train_test_split(\n",
    "    data, test_size=1/10, random_state=179)\n",
    "\n",
    "train_dataset, valid_dataset = train_test_split(\n",
    "    train_dataset, test_size=1/9, random_state=179)\n",
    "\n",
    "\n",
    "'''\n",
    "Tokenization\n",
    "'''\n",
    "import torchtext\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from collections import Counter\n",
    "\n",
    "# tokenizer type\n",
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "\n",
    "# vocab\n",
    "counter = Counter()\n",
    "for (qry, doc) in train_dataset:\n",
    "  counter.update(tokenizer(doc))\n",
    "\n",
    "print('Counts:')\n",
    "print('Information:', counter['information'], 'School:', counter['school'],'\\n')\n",
    "\n",
    "#define vocab\n",
    "vocab = torchtext.vocab.Vocab(counter, max_size=10000,  specials=('<pad>', '<unk>'), specials_first=True)\n",
    "print(\"\\nVocab size:\",len(vocab))\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "Dataloader\n",
    "'''\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "#defined maximum query and doc length\n",
    "max_doc_len = 50\n",
    "max_query_len = 200\n",
    "\n",
    "#tokenization function\n",
    "text_pipeline = lambda x: [vocab[token] for token in tokenizer(x)]\n",
    "\n",
    "#padding function\n",
    "num_neg_samples = 1\n",
    "\n",
    "query_padding_pipeline = lambda tokens: [vocab.stoi['<pad>'] for p in range(max_query_len - len(tokens))] + tokens[-max_query_len:]\n",
    "doc_padding_pipeline = lambda tokens: [vocab.stoi['<pad>'] for p in range(max_doc_len - len(tokens))] + tokens[:max_doc_len]\n",
    "negative_sampling_pipeline = lambda neg_doc_list: random.sample(neg_doc_list, num_neg_samples)\n",
    "\n",
    "#collate function for dataloader\n",
    "train_docs = [d[1] for d in train_dataset]\n",
    "def collate_batch(batch):\n",
    "\n",
    "    #initizlize empty lists for query and doc lists\n",
    "    query_list, pos_doc_list, neg_doc_list = [], [], []\n",
    "\n",
    "    for (qry, doc) in batch:\n",
    "\n",
    "        #query -> tokens -> id -> pad to max query length\n",
    "        qry_ = query_padding_pipeline(text_pipeline(qry))\n",
    "\n",
    "        #doc -> tokens -> ids -> pad to max doc length\n",
    "        doc_ = doc_padding_pipeline(text_pipeline(doc))\n",
    "\n",
    "        #negative samples\n",
    "        irr_doc_list = [d for d in train_docs if d!=doc]\n",
    "        neg_docs = negative_sampling_pipeline(irr_doc_list)\n",
    "        neg_docs_ = [doc_padding_pipeline(text_pipeline(d)) for d in neg_docs]\n",
    "\n",
    "        query_list += [qry_ for q in range(num_neg_samples)]\n",
    "        pos_doc_list += [doc_ for d in range(num_neg_samples)]\n",
    "        neg_doc_list += neg_docs_\n",
    "\n",
    "    #shuffle samples\n",
    "    temp = list(zip(query_list, pos_doc_list, neg_doc_list))\n",
    "    random.shuffle(temp)\n",
    "    query_list, pos_doc_list, neg_doc_list = zip(*temp)\n",
    "\n",
    "    #Now we have numbers, load them to tensors and put on GPU\n",
    "    query_list = torch.tensor(query_list, dtype=torch.int64)\n",
    "    pos_doc_list = torch.tensor(pos_doc_list, dtype=torch.int64)\n",
    "    neg_doc_list = torch.tensor(neg_doc_list, dtype=torch.int64)\n",
    "    return query_list.to(device), pos_doc_list.to(device), neg_doc_list.to(device)\n",
    "\n",
    "\n",
    "BATCH_SIZE=128\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE,\n",
    "                              shuffle=True, collate_fn=collate_batch)\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size=BATCH_SIZE,\n",
    "                              shuffle=False, collate_fn=collate_batch)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE,\n",
    "                             shuffle=False, collate_fn=collate_batch)\n",
    "\n",
    "\n",
    "'''\n",
    "Model\n",
    "'''\n",
    "from torch import nn\n",
    "class Ranking_model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Ranking_model, self).__init__()\n",
    "\n",
    "        #embedding matix, convert tokens to vectors\n",
    "        self.embedding = nn.Embedding(num_embeddings=len(vocab), \n",
    "                                      embedding_dim=50, \n",
    "                                      padding_idx=vocab.stoi['<pad>'])\n",
    "\n",
    "        #Exercise:1\n",
    "        #define the LSTM encoder here.\n",
    "        self.encoder = torch.nn.LSTM(input_size=50, hidden_size=50, num_layers=1, batch_first=True)\n",
    "\n",
    "        #feedforward layer\n",
    "        self.nn_layer1 = nn.Linear(in_features=50*2, out_features=1)\n",
    "\n",
    "\n",
    "    def forward(self, qry_tokens, pos_doc_tokens, neg_doc_tokens):\n",
    "\n",
    "        qry_embedded = self.embedding(qry_tokens)\n",
    "        pos_doc_embedded = self.embedding(pos_doc_tokens)\n",
    "        neg_doc_embedded = self.embedding(neg_doc_tokens)\n",
    "\n",
    "\n",
    "        #Exercise:2\n",
    "        #pass the query, positive, and negative document through the encoder\n",
    "        out_qry, (enc_qry_vector, ct) = self.encoder(qry_embedded)\n",
    "        out_pos, (enc_pos_vector, ct) = self.encoder(pos_doc_embedded)\n",
    "        out_neg, (enc_neg_vector, ct) = self.encoder(neg_doc_embedded)\n",
    "\n",
    "\n",
    "        #Exercise:3\n",
    "        #concat query-positive document and query-negative document\n",
    "        #concat_q_pos_doc = torch.cat((enc_qry_vector[-1], enc_pos_vector[-1]), dim=1)\n",
    "        #concat_q_neg_doc = torch.cat((enc_qry_vector[-1], enc_neg_vector[-1]), dim=1)\n",
    "        concat_q_pos_doc = torch.cat((out_qry.mean(dim=1), out_pos.mean(dim=1)), dim=1)\n",
    "        concat_q_neg_doc = torch.cat((out_qry.mean(dim=1), out_neg.mean(dim=1)), dim=1)\n",
    "\n",
    "\n",
    "        #Exercise:4\n",
    "        #feed them to linear layer\n",
    "        pos_score = torch.relu(self.nn_layer1(concat_q_pos_doc))\n",
    "        neg_score = torch.relu(self.nn_layer1(concat_q_neg_doc))\n",
    "\n",
    "        diff = pos_score - neg_score\n",
    "\n",
    "        return diff\n",
    "\n",
    "\n",
    "# Construct our model by instantiating the model class defined above\n",
    "model = Ranking_model()\n",
    "model.to(device) #put it on the device\n",
    "\n",
    "\n",
    "'''\n",
    "Download glove\n",
    "'''\n",
    "from gensim.models import KeyedVectors\n",
    "import gensim.downloader as api\n",
    "\n",
    "try:\n",
    "    print(\"Loading saved word vectors...\")\n",
    "    glove_50dim = KeyedVectors.load(\"./glove_50dim.w2v\")\n",
    "except:\n",
    "    print(\"Downloading word vectors...\")\n",
    "    glove_50dim = api.load(\"glove-wiki-gigaword-50\")\n",
    "    glove_50dim.save('glove_50dim.w2v')\n",
    "\n",
    "print(\"Number of word vectors:\", glove_50dim.vectors.shape)\n",
    "\n",
    "#Initialise model embedding with glove\n",
    "for word in vocab.stoi.keys():\n",
    "    if word in glove_50dim.key_to_index.keys():\n",
    "        word_vec = glove_50dim[word]\n",
    "        model.embedding.weight.data[vocab.stoi[word]] = torch.tensor(word_vec)\n",
    "\n",
    "\n",
    "#function to score documents based on a query\n",
    "#  to be used for a trained model\n",
    "def rank_docs(qry, doc_list):\n",
    "    for doc in doc_list:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            qry_ = torch.tensor([query_padding_pipeline(text_pipeline(qry))], dtype=torch.int64).to(device)\n",
    "            doc_ = torch.tensor([doc_padding_pipeline(text_pipeline(doc))], dtype=torch.int64).to(device)\n",
    "            score = model(qry_, doc_, doc_*0)\n",
    "            print(\"query [{}] to doc [{}] matching score [{}]\\n\".format(qry, doc, score.detach().item()))\n",
    "\n",
    "\n",
    "'''\n",
    "Training and validation\n",
    "'''\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5) #optimizer\n",
    "num_epochs = 10 #epochs\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(\"-->Epoch:{}\".format(epoch))\n",
    "\n",
    "    epoch_train_loss = 0.0\n",
    "    model.train()\n",
    "    for idx, (qry_tokens, pos_doc_tokens, neg_doc_tokens) in enumerate(train_dataloader):\n",
    "\n",
    "        #flush the gradient values\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        #calculate model output\n",
    "        diff = model(qry_tokens, pos_doc_tokens, neg_doc_tokens)\n",
    "\n",
    "        #Exercise:5.1\n",
    "        #write pairwise loss here\n",
    "        loss = torch.mean(torch.log(1+torch.exp(-1.0*diff)))\n",
    "\n",
    "        #backward pass\n",
    "        loss.backward() \n",
    "\n",
    "        #weights update\n",
    "        optimizer.step()\n",
    "\n",
    "        #average train loss\n",
    "        epoch_train_loss += loss.cpu().item()*BATCH_SIZE\n",
    "\n",
    "        print(\"Batch {}/{}, avg. train loss is {}\".format(idx, len(train_dataloader), epoch_train_loss/(idx+1)), end='\\r')\n",
    "\n",
    "\n",
    "    #validation\n",
    "    epoch_val_loss = 0.0\n",
    "    model.eval()\n",
    "    with torch.no_grad(): #weights should not update\n",
    "        for idx, (qry_tokens, pos_doc_tokens, neg_doc_tokens) in enumerate(valid_dataloader):\n",
    "\n",
    "            #formward pass\n",
    "            diff = model(qry_tokens, pos_doc_tokens, neg_doc_tokens) \n",
    "\n",
    "            #Exercise:5.2\n",
    "            epoch_val_loss += torch.mean(torch.log(1+torch.exp(-1.0*diff))) #same loss as in training\n",
    "\n",
    "        print(\"\\nval loss:{}\".format(epoch_val_loss))\n",
    "\n",
    "        qry = \"we study at the university\"\n",
    "        doc1 = \"the school has a good student to teacher ratio\"\n",
    "        doc2 = \"we have has many students\"\n",
    "        doc3 = \"singapore has a decent climate\"\n",
    "\n",
    "        rank_docs(qry, [doc1, doc2, doc3])\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
